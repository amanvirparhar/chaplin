# Default locally hosted model via Ollama.
CHAPLIN_LLM_MODEL=ollama:qwen3:4b

# Optional: override the system prompt that guides the correction model.
# CHAPLIN_LLM_SYSTEM_PROMPT="You are an assistant that..."

# Optional: provider-specific options (JSON string). Quoting avoids shell parsing issues.
# CHAPLIN_LLM_OPTIONS='{"temperature":0.2}'

# Example: switch to Gemini 2.5 Flash (requires llm-gemini plugin and API key below).
# CHAPLIN_LLM_MODEL=gemini-2.5-flash
# LLM_GEMINI_API_KEY=your-gemini-api-key

# Add other provider keys as needed by their respective llm-* plugins.
